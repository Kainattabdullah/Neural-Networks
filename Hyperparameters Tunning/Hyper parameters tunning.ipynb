{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification Problem**"
      ],
      "metadata": {
        "id": "gpfKb6Sjvb0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas scikit-learn tensorflow==2.12.0 bayesian-optimization optuna pyswarm deap"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\nCollecting tensorflow==2.12.0\n  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nCollecting bayesian-optimization\n  Downloading bayesian_optimization-1.5.1-py3-none-any.whl.metadata (16 kB)\nCollecting optuna\n  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\nCollecting pyswarm\n  Downloading pyswarm-0.6.tar.gz (4.3 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting deap\n  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (24.3.25)\nCollecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0)\n  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.64.1)\nRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.11.0)\nRequirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.33)\nCollecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0)\n  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (18.1.1)\nCollecting numpy\n  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.3.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (24.1)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.20.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (71.0.4)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.16.0)\nCollecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0)\n  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\nCollecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0)\n  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (4.12.2)\nCollecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12.0)\n  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.37.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nCollecting colorama<0.5.0,>=0.4.6 (from bayesian-optimization)\n  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\nINFO: pip is looking at multiple versions of bayesian-optimization to determine which version is compatible with other requirements. This could take a while.\nCollecting bayesian-optimization\n  Downloading bayesian_optimization-1.5.0-py3-none-any.whl.metadata (16 kB)\nCollecting alembic>=1.5.0 (from optuna)\n  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\nCollecting colorlog (from optuna)\n  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.35)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\nCollecting Mako (from alembic>=1.5.0->optuna)\n  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.44.0)\nRequirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.33)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\nINFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\nCollecting jax>=0.3.15 (from tensorflow==2.12.0)\n  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\nCollecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12.0)\n  Downloading jaxlib-0.4.31-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\nCollecting jax>=0.3.15 (from tensorflow==2.12.0)\n  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\nCollecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12.0)\n  Downloading jaxlib-0.4.30-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.27.0)\nCollecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.7)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.4)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.1.5)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\nDownloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bayesian_optimization-1.5.0-py3-none-any.whl (28 kB)\nDownloading optuna-4.0.0-py3-none-any.whl (362 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\nDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\nDownloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\nDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\nDownloading jaxlib-0.4.30-cp310-cp310-manylinux2014_x86_64.whl (79.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyswarm\n  Building wheel for pyswarm (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pyswarm: filename=pyswarm-0.6-py3-none-any.whl size=4464 sha256=59d00a643e40e1943977b0a6d108f8464bffb5c6ac42feaabdecc5ec760d2ed6\n  Stored in directory: /root/.cache/pip/wheels/71/67/40/62fa158f497f942277cbab8199b05cb61c571ab324e67ad0d6\nSuccessfully built pyswarm\nInstalling collected packages: wrapt, tensorflow-estimator, numpy, Mako, keras, gast, colorlog, colorama, pyswarm, deap, alembic, optuna, jaxlib, google-auth-oauthlib, tensorboard, jax, bayesian-optimization, tensorflow\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 1.16.0\n    Uninstalling wrapt-1.16.0:\n      Successfully uninstalled wrapt-1.16.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: keras\n    Found existing installation: keras 3.4.1\n    Uninstalling keras-3.4.1:\n      Successfully uninstalled keras-3.4.1\n  Attempting uninstall: gast\n    Found existing installation: gast 0.6.0\n    Uninstalling gast-0.6.0:\n      Successfully uninstalled gast-0.6.0\n  Attempting uninstall: jaxlib\n    Found existing installation: jaxlib 0.4.33\n    Uninstalling jaxlib-0.4.33:\n      Successfully uninstalled jaxlib-0.4.33\n  Attempting uninstall: google-auth-oauthlib\n    Found existing installation: google-auth-oauthlib 1.2.1\n    Uninstalling google-auth-oauthlib-1.2.1:\n      Successfully uninstalled google-auth-oauthlib-1.2.1\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.17.0\n    Uninstalling tensorboard-2.17.0:\n      Successfully uninstalled tensorboard-2.17.0\n  Attempting uninstall: jax\n    Found existing installation: jax 0.4.33\n    Uninstalling jax-0.4.33:\n      Successfully uninstalled jax-0.4.33\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.17.0\n    Uninstalling tensorflow-2.17.0:\n      Successfully uninstalled tensorflow-2.17.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nalbucore 0.0.16 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\nalbumentations 1.4.15 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nbigframes 1.18.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\nchex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\npandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.23.5 which is incompatible.\ntf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.12.0 which is incompatible.\nxarray 2024.9.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Mako-1.3.5 alembic-1.13.3 bayesian-optimization-1.5.0 colorama-0.4.6 colorlog-6.8.2 deap-1.4.1 gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 optuna-4.0.0 pyswarm-0.6 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "jax",
                  "jaxlib",
                  "keras",
                  "numpy",
                  "tensorflow",
                  "wrapt"
                ]
              },
              "id": "0ff931f6999842bcb66c6473a8836367"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FeJ1fnUH3C82",
        "outputId": "40c39f69-7af5-4f50-f020-5258d50187d8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "import optuna\n",
        "# ============================\n",
        "# Load the Iris dataset\n",
        "# ============================\n",
        "iris = load_iris()  # Load the dataset\n",
        "X, y = iris.data, iris.target  # Features and target variable\n",
        "\n",
        "# ============================\n",
        "# Split the dataset into training and testing sets\n",
        "# ============================\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "random_state=42)\n",
        "\n",
        "# ============================\n",
        "\n",
        "\n",
        "# Function to create the Keras model\n",
        "# ============================\n",
        "def create_classification_model(optimizer='adam', activation='relu',\n",
        "neurons=10):\n",
        "    model = Sequential()  # Initialize a sequential model\n",
        "    model.add(Dense(neurons, input_dim=X_train.shape[1],activation=activation))  # Hidden layer\n",
        "    model.add(Dense(3, activation='softmax'))  # Output layer for 3 classes (Iris species)\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "    optimizer=optimizer, metrics=['accuracy'])  # Compile the model\n",
        "    return model  # Return the created model\n",
        "# ============================\n",
        "# Wrap the model with KerasClassifier\n",
        "# ============================\n",
        "model = KerasClassifier(build_fn=create_classification_model, verbose=0)\n",
        "# Wrap the Keras model for scikit-learn\n",
        "# ============================\n",
        "# Hyperparameter grid for optimization\n",
        "# ============================\n",
        "param_grid = {\n",
        "    'optimizer': ['adam', 'rmsprop', 'sgd'],  # Different optimizers to test\n",
        "    'activation': ['relu', 'tanh'],  # Activation functions to test\n",
        "    'neurons': [10, 20, 30],  # Number of neurons in the hidden layer\n",
        "    'batch_size': [5, 10],  # Batch sizes to test\n",
        "    'epochs': [10, 20],  # Number of epochs for training\n",
        "}\n",
        "\n",
        "# ============================\n",
        "# 1. Grid Search\n",
        "# ============================\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid,\n",
        "scoring='accuracy', cv=3)  # Initialize Grid Search\n",
        "grid.fit(X_train, y_train)  # Fit the model on training data\n",
        "print(\"Best parameters from Grid Search:\", grid.best_params_)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "<ipython-input-5-f8845e048c0a>:37: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n  model = KerasClassifier(build_fn=create_classification_model, verbose=0)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 9ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x78ddb34448b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2/2 [==============================] - 0s 9ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x78ddb3445d80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 4ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 10ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 10ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 4ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 10ms/step\n2/2 [==============================] - 0s 10ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 10ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 4ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 4ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 4ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 11ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 7ms/step\nBest parameters from Grid Search: {'activation': 'relu', 'batch_size': 10, 'epochs': 20, 'neurons': 20, 'optimizer': 'sgd'}\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkmP8dAC0bDp",
        "outputId": "45362a33-711d-4b6b-b9b6-3f1d98173568"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 2. Random Search\n",
        "# ============================\n",
        "random_search = RandomizedSearchCV(estimator=model,\n",
        "param_distributions=param_grid, n_iter=10, scoring='accuracy', cv=3)  # Initialize Random Search\n",
        "random_search.fit(X_train, y_train)  # Fit the model on training data\n",
        "print(\"Best parameters from Random Search:\", random_search.best_params_)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2/2 [==============================] - 0s 31ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 12ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 7ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 10ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 11ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 8ms/step\n2/2 [==============================] - 0s 5ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 6ms/step\n2/2 [==============================] - 0s 9ms/step\n2/2 [==============================] - 0s 11ms/step\n2/2 [==============================] - 0s 8ms/step\nBest parameters from Random Search: {'optimizer': 'sgd', 'neurons': 20, 'epochs': 20, 'batch_size': 5, 'activation': 'tanh'}\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-E-f5rC1hk6",
        "outputId": "f83f0110-aa46-4a2a-b019-fb1face48625"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 3. Hyperband (using Optuna)\n",
        "# ============================\n",
        "def objective(trial):\n",
        "    # Define hyperparameters to optimize\n",
        "    optimizer = trial.suggest_categorical('optimizer', ['adam', 'rmsprop', 'sgd'])  # Optimizer\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])  # Activation function\n",
        "    neurons = trial.suggest_int('neurons', 10, 30)  # Number of neurons\n",
        "    batch_size = trial.suggest_categorical('batch_size', [5, 10])  # Batch size\n",
        "    # Create the model with the suggested hyperparameters\n",
        "    model = create_classification_model(optimizer=optimizer, activation=activation,neurons=neurons)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, batch_size=batch_size, epochs=10, verbose=0)\n",
        "    # Evaluate the model on the test set\n",
        "    accuracy = model.evaluate(X_test, y_test, verbose=0)[1]  # Get the accuracy\n",
        "    return accuracy  # Return the accuracy for optimization\n",
        "\n",
        "# Create and optimize the study\n",
        "study = optuna.create_study(direction='maximize')  # Create a study to maximize accuracy\n",
        "study.optimize(objective, n_trials=10)  # Optimize the objective function over 10 trials\n",
        "print(\"Best parameters from Hyperband (Optuna):\", study.best_params)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-24 07:22:04,999] A new study created in memory with name: no-name-6a2005ca-a8b7-4a8d-a32b-5230017ed019\n[I 2024-09-24 07:22:06,723] Trial 0 finished with value: 0.7333333492279053 and parameters: {'optimizer': 'sgd', 'activation': 'relu', 'neurons': 26, 'batch_size': 5}. Best is trial 0 with value: 0.7333333492279053.\n[I 2024-09-24 07:22:08,446] Trial 1 finished with value: 0.800000011920929 and parameters: {'optimizer': 'adam', 'activation': 'tanh', 'neurons': 20, 'batch_size': 5}. Best is trial 1 with value: 0.800000011920929.\n[I 2024-09-24 07:22:09,778] Trial 2 finished with value: 0.9666666388511658 and parameters: {'optimizer': 'sgd', 'activation': 'relu', 'neurons': 23, 'batch_size': 5}. Best is trial 2 with value: 0.9666666388511658.\n[I 2024-09-24 07:22:12,078] Trial 3 finished with value: 0.9666666388511658 and parameters: {'optimizer': 'rmsprop', 'activation': 'relu', 'neurons': 10, 'batch_size': 5}. Best is trial 2 with value: 0.9666666388511658.\nWARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x78dd91643760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n[I 2024-09-24 07:22:13,861] Trial 4 finished with value: 0.699999988079071 and parameters: {'optimizer': 'adam', 'activation': 'tanh', 'neurons': 11, 'batch_size': 10}. Best is trial 2 with value: 0.9666666388511658.\nWARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x78dd968d4e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n[I 2024-09-24 07:22:15,453] Trial 5 finished with value: 0.8333333134651184 and parameters: {'optimizer': 'adam', 'activation': 'tanh', 'neurons': 16, 'batch_size': 5}. Best is trial 2 with value: 0.9666666388511658.\n[I 2024-09-24 07:22:16,488] Trial 6 finished with value: 0.8999999761581421 and parameters: {'optimizer': 'sgd', 'activation': 'relu', 'neurons': 26, 'batch_size': 10}. Best is trial 2 with value: 0.9666666388511658.\n[I 2024-09-24 07:22:17,617] Trial 7 finished with value: 0.6333333253860474 and parameters: {'optimizer': 'rmsprop', 'activation': 'tanh', 'neurons': 19, 'batch_size': 10}. Best is trial 2 with value: 0.9666666388511658.\n[I 2024-09-24 07:22:18,962] Trial 8 finished with value: 0.800000011920929 and parameters: {'optimizer': 'adam', 'activation': 'relu', 'neurons': 24, 'batch_size': 10}. Best is trial 2 with value: 0.9666666388511658.\n[I 2024-09-24 07:22:19,869] Trial 9 finished with value: 1.0 and parameters: {'optimizer': 'sgd', 'activation': 'tanh', 'neurons': 20, 'batch_size': 10}. Best is trial 9 with value: 1.0.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Best parameters from Hyperband (Optuna): {'optimizer': 'sgd', 'activation': 'tanh', 'neurons': 20, 'batch_size': 10}\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxG11EkR1vrw",
        "outputId": "51bd3be5-840c-437f-da33-53384a138458"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# ============================\n",
        "# Function to optimize with Bayesian Optimization\n",
        "# ============================\n",
        "def bayesian_opt(neurons, optimizer_idx, activation_idx, batch_size):\n",
        "    # Define options for optimizers and activation functions\n",
        "    optimizer_options = ['adam', 'rmsprop', 'sgd']  # List of optimizer options\n",
        "    activation_options = ['relu', 'tanh']  # List of activation function options\n",
        "\n",
        "    # Map indices to actual optimizer and activation function\n",
        "    optimizer = optimizer_options[int(optimizer_idx)]  # Select optimizer based on index\n",
        "    activation = activation_options[int(activation_idx)]  # Select activation function based on index\n",
        "    # Create and compile the Keras model\n",
        "    model = create_classification_model(optimizer=optimizer, activation=activation, neurons=int(neurons))\n",
        "\n",
        "    # Train the model on the training set\n",
        "    model.fit(X_train, y_train, batch_size=int(batch_size), epochs=10, verbose=0)\n",
        "\n",
        "    # Evaluate the model on the test set and return the accuracy\n",
        "    accuracy = model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "    return accuracy  # Return the achieved accuracy\n",
        "\n",
        "# ============================\n",
        "# Bounds for Bayesian Optimization\n",
        "# ============================\n",
        "pbounds = {\n",
        "    'neurons': (10, 30),  # Number of neurons in the hidden layer\n",
        "    'optimizer_idx': (0, 2),  # Indices for optimizers (0: 'adam', 1: 'rmsprop', 2: 'sgd')\n",
        "    'activation_idx': (0, 1),  # Indices for activation functions (0: 'relu', 1: 'tanh')\n",
        "    'batch_size': (5, 10),  # Batch size to test\n",
        "}\n",
        "\n",
        "# ============================\n",
        "# Initialize and run Bayesian Optimization\n",
        "# ============================\n",
        "optimizer = BayesianOptimization(f=bayesian_opt, pbounds=pbounds, random_state=1)  # Create the optimizer\n",
        "optimizer.maximize(init_points=5, n_iter=10)  # Run optimization with initial points and iterations\n",
        "print(\"Best parameters from BO-GP:\", optimizer.max)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "|   iter    |  target   | activa... | batch_... |  neurons  | optimi... |\n-------------------------------------------------------------------------\n| \u001b[30m1         | \u001b[30m0.4       | \u001b[30m0.417     | \u001b[30m8.602     | \u001b[30m10.0      | \u001b[30m0.6047    |\n| \u001b[35m2         | \u001b[35m0.8333    | \u001b[35m0.1468    | \u001b[35m5.462     | \u001b[35m13.73     | \u001b[35m0.6911    |\n| \u001b[30m3         | \u001b[30m0.8333    | \u001b[30m0.3968    | \u001b[30m7.694     | \u001b[30m18.38     | \u001b[30m1.37      |\n| \u001b[30m4         | \u001b[30m0.3667    | \u001b[30m0.2045    | \u001b[30m9.391     | \u001b[30m10.55     | \u001b[30m1.341     |\n| \u001b[30m5         | \u001b[30m0.7       | \u001b[30m0.4173    | \u001b[30m7.793     | \u001b[30m12.81     | \u001b[30m0.3962    |\n| \u001b[35m6         | \u001b[35m0.9667    | \u001b[35m1.0       | \u001b[35m5.0       | \u001b[35m22.99     | \u001b[35m0.0       |\n| \u001b[30m7         | \u001b[30m0.9       | \u001b[30m0.0       | \u001b[30m7.665     | \u001b[30m28.18     | \u001b[30m0.0       |\n| \u001b[30m8         | \u001b[30m0.7333    | \u001b[30m1.0       | \u001b[30m10.0      | \u001b[30m24.36     | \u001b[30m2.0       |\n| \u001b[35m9         | \u001b[35m1.0       | \u001b[35m1.0       | \u001b[35m5.0       | \u001b[35m30.0      | \u001b[35m2.0       |\n| \u001b[30m10        | \u001b[30m0.9       | \u001b[30m0.0       | \u001b[30m5.0       | \u001b[30m26.23     | \u001b[30m2.0       |\n| \u001b[30m11        | \u001b[30m0.9       | \u001b[30m1.0       | \u001b[30m5.0       | \u001b[30m30.0      | \u001b[30m0.0       |\n| \u001b[30m12        | \u001b[30m0.8       | \u001b[30m1.0       | \u001b[30m10.0      | \u001b[30m30.0      | \u001b[30m2.0       |\n| \u001b[30m13        | \u001b[30m0.9       | \u001b[30m0.0       | \u001b[30m5.0       | \u001b[30m19.12     | \u001b[30m0.0       |\n| \u001b[30m14        | \u001b[30m0.8667    | \u001b[30m1.0       | \u001b[30m5.0       | \u001b[30m21.32     | \u001b[30m2.0       |\n| \u001b[30m15        | \u001b[30m0.8       | \u001b[30m0.0       | \u001b[30m6.367     | \u001b[30m30.0      | \u001b[30m2.0       |\n=========================================================================\nBest parameters from BO-GP: {'target': 1.0, 'params': {'activation_idx': 1.0, 'batch_size': 5.0, 'neurons': 30.0, 'optimizer_idx': 2.0}}\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypOPCh1x6KJR",
        "outputId": "03031add-2a01-4835-f427-d0835e1339b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyswarm"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: pyswarm in /usr/local/lib/python3.10/dist-packages (0.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyswarm) (1.23.5)\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9rF63bC7hw-",
        "outputId": "5bb7b4a1-d753-4f6a-8a7d-fe4cfbbacb80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyswarm import pso\n",
        "# ============================\n",
        "# Particle Swarm Optimization (PSO)\n",
        "# ============================\n",
        "from pyswarm import pso  # Import the PSO function\n",
        "\n",
        "def pso_objective(params):\n",
        "    # Extract parameters from the input array\n",
        "    neurons, batch_size = int(params[0]), int(params[1])\n",
        "    optimizer = 'adam'  # Fixed optimizer for simplification\n",
        "    activation = 'relu'  # Fixed activation for simplification\n",
        "    # Create and compile the Keras model\n",
        "    model = create_classification_model(optimizer=optimizer, activation=activation, neurons=neurons)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, batch_size=batch_size, epochs=10, verbose=0)\n",
        "\n",
        "    # Evaluate the model on the test set and return the negative accuracy (to minimize)\n",
        "    accuracy = model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "    return -accuracy  # Return negative accuracy for minimization\n",
        "\n",
        "# Define bounds for the PSO parameters\n",
        "lb = [10, 5]  # Lower bounds for neurons and batch size\n",
        "ub = [30, 10]  # Upper bounds for neurons and batch size\n",
        "\n",
        "# Run Particle Swarm Optimization\n",
        "best_params, _ = pso(pso_objective, lb, ub, swarmsize=10, maxiter=5)  # Optimize with PSO\n",
        "print(\"Best parameters from PSO:\", best_params)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Stopping search: maximum iterations reached --> 5\nBest parameters from PSO: [13.51565461  7.88549923]\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO8Jllk86lk7",
        "outputId": "fa2fd724-1536-461c-ce3f-f8c5e8ca8d0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Forecasting California Housing Prices with LSTM, GRU, RNN,and CNN Models (Sequential Problem)**"
      ],
      "metadata": {
        "id": "OV67e5_TvkkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, GRU, Dense, Dropout, SimpleRNN, Conv1D, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "import optuna\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# ============================\n",
        "# Load and Prepare the Dataset\n",
        "# ============================\n",
        "# Load California housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target variable\n",
        "\n",
        "# Create sequences for time series modeling\n",
        "def create_sequences(X, y, time_steps=10):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        X_seq.append(X[i:i + time_steps])  # Append sequences of features\n",
        "        y_seq.append(y[i + time_steps])  # Append the target for the next time step\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Create sequences\n",
        "X_seq, y_seq = create_sequences(X, y)\n",
        "\n",
        "# Reshape X for LSTM/GRU/RNN: (samples, time steps, features)\n",
        "X_seq = X_seq.reshape((X_seq.shape[0], X_seq.shape[1], X_seq.shape[2]))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the target variable for better training performance\n",
        "scaler = StandardScaler()\n",
        "y_train = scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "y_test = scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Early stopping callback to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# ============================\n",
        "# Model Creation Functions\n",
        "# ============================\n",
        "# Function to create an LSTM model\n",
        "def create_lstm_model(optimizer='adam', lstm_units=50, dropout_rate=0.2):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(lstm_units, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(dropout_rate))  # Dropout for regularization\n",
        "    model.add(Dense(1))  # Output layer for regression\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)  # Compile model\n",
        "    return model\n",
        "\n",
        "# Function to create a GRU model\n",
        "def create_gru_model(optimizer='adam', gru_units=50, dropout_rate=0.2):\n",
        "    model = Sequential()\n",
        "    model.add(GRU(gru_units, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(dropout_rate))  # Dropout for regularization\n",
        "    model.add(Dense(1))  # Output layer for regression\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)  # Compile model\n",
        "    return model\n",
        "\n",
        "# Function to create a Simple RNN model\n",
        "def create_rnn_model(optimizer='adam', rnn_units=50, dropout_rate=0.2):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(rnn_units, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(dropout_rate))  # Dropout for regularization\n",
        "    model.add(Dense(1))  # Output layer for regression\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)  # Compile model\n",
        "    return model\n",
        "\n",
        "# Function to create a 1D CNN model\n",
        "def create_cnn_model(optimizer='adam', filters=32, kernel_size=3, dropout_rate=0.2):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(dropout_rate))  # Dropout for regularization\n",
        "    model.add(Flatten())  # Flatten for the Dense layer\n",
        "    model.add(Dense(1))  # Output layer for regression\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)  # Compile model\n",
        "    return model\n",
        "\n",
        "# ============================\n",
        "# Model Evaluation Function\n",
        "# ============================\n",
        "def evaluate_model(model):\n",
        "    predictions = model.predict(X_test)  # Generate predictions on test set\n",
        "    predictions = scaler.inverse_transform(predictions)  # Inverse scaling to original scale\n",
        "    mse = mean_squared_error(y_test, predictions)  # Calculate Mean Squared Error\n",
        "    r2 = r2_score(y_test, predictions)  # Calculate R² score\n",
        "    accuracy_percentage = r2 * 100  # Convert R² to percentage\n",
        "    return mse, accuracy_percentage  # Return both metrics\n",
        "\n",
        "# ============================\n",
        "# Hyperparameter Tuning Functions\n",
        "# ============================\n",
        "# Hyperparameter tuning for LSTM\n",
        "def objective_lstm(trial):\n",
        "    optimizer = trial.suggest_categorical('optimizer', ['adam', 'sgd'])  # Choose optimizer\n",
        "    lstm_units = trial.suggest_int('lstm_units', 10, 100)  # Number of LSTM units\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)  # Dropout rate\n",
        "\n",
        "    model = create_lstm_model(optimizer=optimizer, lstm_units=lstm_units, dropout_rate=dropout_rate)\n",
        "    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, callbacks=[early_stopping])  # Fit model\n",
        "\n",
        "    mse, accuracy = evaluate_model(model)  # Evaluate model\n",
        "    return mse  # Minimize MSE\n",
        "\n",
        "# Hyperparameter tuning for GRU\n",
        "def objective_gru(trial):\n",
        "    optimizer = trial.suggest_categorical('optimizer', ['adam', 'sgd'])  # Choose optimizer\n",
        "    gru_units = trial.suggest_int('gru_units', 10, 100)  # Number of GRU units\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)  # Dropout rate\n",
        "\n",
        "    model = create_gru_model(optimizer=optimizer, gru_units=gru_units, dropout_rate=dropout_rate)\n",
        "    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, callbacks=[early_stopping])  # Fit model\n",
        "\n",
        "    mse, accuracy = evaluate_model(model)  # Evaluate model\n",
        "    return mse  # Minimize MSE\n",
        "\n",
        "# Hyperparameter tuning for RNN\n",
        "def objective_rnn(trial):\n",
        "    optimizer = trial.suggest_categorical('optimizer', ['adam', 'sgd'])  # Choose optimizer\n",
        "    rnn_units = trial.suggest_int('rnn_units', 10, 100)  # Number of RNN units\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)  # Dropout rate\n",
        "\n",
        "    model = create_rnn_model(optimizer=optimizer, rnn_units=rnn_units, dropout_rate=dropout_rate)\n",
        "    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, callbacks=[early_stopping])  # Fit model\n",
        "\n",
        "    mse, accuracy = evaluate_model(model)  # Evaluate model\n",
        "    return mse  # Minimize MSE\n",
        "\n",
        "# Hyperparameter tuning for CNN\n",
        "def objective_cnn(trial):\n",
        "    optimizer = trial.suggest_categorical('optimizer', ['adam', 'sgd'])  # Choose optimizer\n",
        "    filters = trial.suggest_int('filters', 16, 64)  # Number of filters\n",
        "    kernel_size = trial.suggest_int('kernel_size', 2, 5)  # Kernel size\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)  # Dropout rate\n",
        "\n",
        "    model = create_cnn_model(optimizer=optimizer, filters=filters, kernel_size=kernel_size, dropout_rate=dropout_rate)\n",
        "    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, callbacks=[early_stopping])  # Fit model\n",
        "\n",
        "    mse, accuracy = evaluate_model(model)  # Evaluate model\n",
        "    return mse  # Minimize MSE\n",
        "\n",
        "# ============================\n",
        "# Optuna Studies for Hyperparameter Optimization\n",
        "# ============================\n",
        "# Optuna study for LSTM\n",
        "study_lstm = optuna.create_study(direction='minimize')\n",
        "study_lstm.optimize(objective_lstm, n_trials=5)  # Optimize LSTM parameters\n",
        "best_lstm_mse, best_lstm_accuracy = evaluate_model(create_lstm_model(**study_lstm.best_params))\n",
        "print(\"Best parameters for LSTM:\", study_lstm.best_params)\n",
        "print(f\"LSTM - MSE: {best_lstm_mse:.2f}, Accuracy Percentage: {best_lstm_accuracy:.2f}%\")\n",
        "\n",
        "# Optuna study for GRU\n",
        "study_gru = optuna.create_study(direction='minimize')\n",
        "study_gru.optimize(objective_gru, n_trials=5)  # Optimize GRU parameters\n",
        "best_gru_mse, best_gru_accuracy = evaluate_model(create_gru_model(**study_gru.best_params))\n",
        "print(\"Best parameters for GRU:\", study_gru.best_params)\n",
        "print(f\"GRU - MSE: {best_gru_mse:.2f}, Accuracy Percentage: {best_gru_accuracy:.2f}%\")\n",
        "\n",
        "# Optuna study for\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 16:36:23,577] A new study created in memory with name: no-name-e88a1b2f-d869-4153-97da-b1f2302bb8b3\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/20\n516/516 [==============================] - 12s 10ms/step - loss: 1.0129\nEpoch 2/20\n516/516 [==============================] - 4s 8ms/step - loss: 0.9959\nEpoch 3/20\n516/516 [==============================] - 4s 8ms/step - loss: 0.9952\nEpoch 4/20\n516/516 [==============================] - 4s 7ms/step - loss: 0.9960\nEpoch 5/20\n516/516 [==============================] - 4s 8ms/step - loss: 0.9965\nEpoch 6/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.9964\n129/129 [==============================] - 1s 3ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 16:36:59,546] Trial 0 finished with value: 5.110640497218157 and parameters: {'optimizer': 'sgd', 'lstm_units': 16, 'dropout_rate': 0.18557598946423323}. Best is trial 0 with value: 5.110640497218157.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/20\n516/516 [==============================] - 7s 8ms/step - loss: 1.0682\nEpoch 2/20\n516/516 [==============================] - 5s 10ms/step - loss: 1.0241\nEpoch 3/20\n516/516 [==============================] - 4s 8ms/step - loss: 1.0072\nEpoch 4/20\n516/516 [==============================] - 4s 8ms/step - loss: 1.0021\nEpoch 5/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.9935\nEpoch 6/20\n516/516 [==============================] - 7s 14ms/step - loss: 0.9439\nEpoch 7/20\n516/516 [==============================] - 7s 13ms/step - loss: 0.8148\nEpoch 8/20\n516/516 [==============================] - 7s 13ms/step - loss: 0.7795\nEpoch 9/20\n516/516 [==============================] - 5s 11ms/step - loss: 0.7312\nEpoch 10/20\n516/516 [==============================] - 8s 16ms/step - loss: 0.7323\nEpoch 11/20\n516/516 [==============================] - 7s 13ms/step - loss: 0.7164\nEpoch 12/20\n516/516 [==============================] - 7s 14ms/step - loss: 0.7235\nEpoch 13/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.7069\nEpoch 14/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.7075\nEpoch 15/20\n516/516 [==============================] - 4s 9ms/step - loss: 0.7055\nEpoch 16/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.7208\nEpoch 17/20\n516/516 [==============================] - 8s 16ms/step - loss: 0.7072\nEpoch 18/20\n516/516 [==============================] - 4s 8ms/step - loss: 0.6945\nEpoch 19/20\n516/516 [==============================] - 4s 8ms/step - loss: 0.6885\nEpoch 20/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.6949\n129/129 [==============================] - 1s 3ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 16:38:55,172] Trial 1 finished with value: 4.278432363521988 and parameters: {'optimizer': 'adam', 'lstm_units': 18, 'dropout_rate': 0.10831512149766637}. Best is trial 1 with value: 4.278432363521988.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/20\n516/516 [==============================] - 8s 10ms/step - loss: 1.1007\nEpoch 2/20\n516/516 [==============================] - 6s 13ms/step - loss: 0.9737\nEpoch 3/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.8224\nEpoch 4/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.7119\nEpoch 5/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.6536\nEpoch 6/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.6140\nEpoch 7/20\n516/516 [==============================] - 6s 13ms/step - loss: 0.5819\nEpoch 8/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.5779\nEpoch 9/20\n516/516 [==============================] - 5s 11ms/step - loss: 0.5695\nEpoch 10/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.5597\nEpoch 11/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.5549\nEpoch 12/20\n516/516 [==============================] - 8s 15ms/step - loss: 0.5705\nEpoch 13/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.5561\nEpoch 14/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.5421\nEpoch 15/20\n516/516 [==============================] - 6s 12ms/step - loss: 0.5513\nEpoch 16/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.5493\nEpoch 17/20\n516/516 [==============================] - 6s 12ms/step - loss: 0.5431\n129/129 [==============================] - 1s 6ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 16:41:21,275] Trial 2 finished with value: 4.190511365423602 and parameters: {'optimizer': 'adam', 'lstm_units': 76, 'dropout_rate': 0.14992110749872856}. Best is trial 2 with value: 4.190511365423602.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/20\n516/516 [==============================] - 8s 11ms/step - loss: 1.0291\nEpoch 2/20\n516/516 [==============================] - 7s 14ms/step - loss: 1.0055\nEpoch 3/20\n516/516 [==============================] - 6s 11ms/step - loss: 1.0074\nEpoch 4/20\n516/516 [==============================] - 8s 15ms/step - loss: 1.0049\nEpoch 5/20\n516/516 [==============================] - 6s 11ms/step - loss: 1.0106\nEpoch 6/20\n516/516 [==============================] - 7s 13ms/step - loss: 1.0099\nEpoch 7/20\n516/516 [==============================] - 6s 12ms/step - loss: 1.0067\n129/129 [==============================] - 1s 5ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 16:42:47,827] Trial 3 finished with value: 5.8801825514026405 and parameters: {'optimizer': 'sgd', 'lstm_units': 93, 'dropout_rate': 0.24611494732946917}. Best is trial 2 with value: 4.190511365423602.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/20\n516/516 [==============================] - 8s 12ms/step - loss: 1.0622\nEpoch 2/20\n516/516 [==============================] - 4s 9ms/step - loss: 0.9183\nEpoch 3/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.7927\nEpoch 4/20\n516/516 [==============================] - 6s 12ms/step - loss: 0.7132\nEpoch 5/20\n516/516 [==============================] - 4s 9ms/step - loss: 0.6840\nEpoch 6/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.6737\nEpoch 7/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.6446\nEpoch 8/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.6487\nEpoch 9/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.6331\nEpoch 10/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.6185\nEpoch 11/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.6263\nEpoch 12/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.6142\nEpoch 13/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.6144\nEpoch 14/20\n516/516 [==============================] - 4s 9ms/step - loss: 0.6182\nEpoch 15/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.6314\n129/129 [==============================] - 1s 4ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 16:44:13,258] Trial 4 finished with value: 4.676813063079683 and parameters: {'optimizer': 'adam', 'lstm_units': 58, 'dropout_rate': 0.354447452689444}. Best is trial 2 with value: 4.190511365423602.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "129/129 [==============================] - 1s 4ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 16:44:14,675] A new study created in memory with name: no-name-516e9029-e809-4887-bce0-9b7d43a8c880\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Best parameters for LSTM: {'optimizer': 'adam', 'lstm_units': 76, 'dropout_rate': 0.14992110749872856}\nLSTM - MSE: 3.01, Accuracy Percentage: -194.06%\nEpoch 1/20\n516/516 [==============================] - 8s 10ms/step - loss: 1.3340\nEpoch 2/20\n516/516 [==============================] - 5s 9ms/step - loss: 1.0326\nEpoch 3/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.9785\nEpoch 4/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.9484\nEpoch 5/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.9015\nEpoch 6/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.9071\nEpoch 7/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.8740\nEpoch 8/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.8625\nEpoch 9/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.8451\nEpoch 10/20\n516/516 [==============================] - 4s 9ms/step - loss: 0.7766\nEpoch 11/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.7545\nEpoch 12/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.7173\nEpoch 13/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.7358\nEpoch 14/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.7085\nEpoch 15/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.7139\nEpoch 16/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.6880\nEpoch 17/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.6761\nEpoch 18/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.6658\nEpoch 19/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.6605\nEpoch 20/20\n516/516 [==============================] - 6s 12ms/step - loss: 0.6710\n129/129 [==============================] - 1s 3ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 16:46:00,177] Trial 0 finished with value: 4.727915348021391 and parameters: {'optimizer': 'adam', 'gru_units': 52, 'dropout_rate': 0.34312407337373063}. Best is trial 0 with value: 4.727915348021391.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/20\n516/516 [==============================] - 7s 8ms/step - loss: 1.0607\nEpoch 2/20\n516/516 [==============================] - 6s 11ms/step - loss: 1.0107\nEpoch 3/20\n516/516 [==============================] - 4s 8ms/step - loss: 1.0085\nEpoch 4/20\n516/516 [==============================] - 5s 9ms/step - loss: 1.0100\nEpoch 5/20\n516/516 [==============================] - 6s 11ms/step - loss: 1.0109\nEpoch 6/20\n516/516 [==============================] - 5s 9ms/step - loss: 1.0081\nEpoch 7/20\n516/516 [==============================] - 5s 10ms/step - loss: 1.0074\nEpoch 8/20\n516/516 [==============================] - 5s 10ms/step - loss: 1.0076\nEpoch 9/20\n516/516 [==============================] - 4s 9ms/step - loss: 1.0062\nEpoch 10/20\n516/516 [==============================] - 5s 10ms/step - loss: 1.0074\nEpoch 11/20\n516/516 [==============================] - 5s 9ms/step - loss: 1.0064\nEpoch 12/20\n516/516 [==============================] - 4s 8ms/step - loss: 1.0069\n129/129 [==============================] - 1s 3ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 16:47:02,604] Trial 1 finished with value: 5.208133763485554 and parameters: {'optimizer': 'sgd', 'gru_units': 38, 'dropout_rate': 0.2763214039519473}. Best is trial 0 with value: 4.727915348021391.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/20\n516/516 [==============================] - 9s 9ms/step - loss: 1.0560\nEpoch 2/20\n516/516 [==============================] - 4s 8ms/step - loss: 1.0050\nEpoch 3/20\n516/516 [==============================] - 6s 11ms/step - loss: 1.0024\nEpoch 4/20\n516/516 [==============================] - 4s 9ms/step - loss: 1.0033\nEpoch 5/20\n516/516 [==============================] - 4s 8ms/step - loss: 1.0037\nEpoch 6/20\n516/516 [==============================] - 6s 11ms/step - loss: 1.0033\n129/129 [==============================] - 1s 3ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 16:47:37,159] Trial 2 finished with value: 4.560185350210599 and parameters: {'optimizer': 'sgd', 'gru_units': 25, 'dropout_rate': 0.39373064558983684}. Best is trial 2 with value: 4.560185350210599.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/20\n516/516 [==============================] - 7s 9ms/step - loss: 1.0912\nEpoch 2/20\n516/516 [==============================] - 5s 11ms/step - loss: 1.0325\nEpoch 3/20\n516/516 [==============================] - 5s 9ms/step - loss: 1.0080\nEpoch 4/20\n516/516 [==============================] - 4s 9ms/step - loss: 1.0011\nEpoch 5/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.9933\nEpoch 6/20\n516/516 [==============================] - 4s 8ms/step - loss: 0.9911\nEpoch 7/20\n516/516 [==============================] - 4s 8ms/step - loss: 1.0029\nEpoch 8/20\n516/516 [==============================] - 5s 11ms/step - loss: 0.9783\nEpoch 9/20\n516/516 [==============================] - 4s 9ms/step - loss: 0.9066\nEpoch 10/20\n516/516 [==============================] - 4s 9ms/step - loss: 0.8368\nEpoch 11/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.7849\nEpoch 12/20\n516/516 [==============================] - 4s 9ms/step - loss: 0.7749\nEpoch 13/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.7530\nEpoch 14/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.7463\nEpoch 15/20\n516/516 [==============================] - 4s 8ms/step - loss: 0.7352\nEpoch 16/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.7313\nEpoch 17/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.7225\nEpoch 18/20\n516/516 [==============================] - 4s 8ms/step - loss: 0.7240\nEpoch 19/20\n516/516 [==============================] - 5s 9ms/step - loss: 0.7163\nEpoch 20/20\n516/516 [==============================] - 5s 10ms/step - loss: 0.7108\n129/129 [==============================] - 1s 3ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 16:49:16,376] Trial 3 finished with value: 5.97705668216411 and parameters: {'optimizer': 'adam', 'gru_units': 13, 'dropout_rate': 0.11033658792476855}. Best is trial 2 with value: 4.560185350210599.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/20\n516/516 [==============================] - 9s 12ms/step - loss: 1.2158\nEpoch 2/20\n516/516 [==============================] - 6s 13ms/step - loss: 1.0072\nEpoch 3/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.8340\nEpoch 4/20\n516/516 [==============================] - 7s 13ms/step - loss: 0.7456\nEpoch 5/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.6747\nEpoch 6/20\n516/516 [==============================] - 7s 14ms/step - loss: 0.6276\nEpoch 7/20\n516/516 [==============================] - 6s 12ms/step - loss: 0.5917\nEpoch 8/20\n516/516 [==============================] - 7s 13ms/step - loss: 0.5796\nEpoch 9/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.5806\nEpoch 10/20\n516/516 [==============================] - 7s 14ms/step - loss: 0.5594\nEpoch 11/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.5915\nEpoch 12/20\n516/516 [==============================] - 6s 12ms/step - loss: 0.5704\nEpoch 13/20\n516/516 [==============================] - 7s 13ms/step - loss: 0.5441\nEpoch 14/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.5440\nEpoch 15/20\n516/516 [==============================] - 7s 14ms/step - loss: 0.5622\nEpoch 16/20\n516/516 [==============================] - 6s 11ms/step - loss: 0.5561\nEpoch 17/20\n516/516 [==============================] - 7s 14ms/step - loss: 0.5639\n129/129 [==============================] - 2s 8ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 16:51:44,281] Trial 4 finished with value: 4.429535527641304 and parameters: {'optimizer': 'adam', 'gru_units': 85, 'dropout_rate': 0.1689673689977081}. Best is trial 4 with value: 4.429535527641304.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "129/129 [==============================] - 1s 4ms/step\nBest parameters for GRU: {'optimizer': 'adam', 'gru_units': 85, 'dropout_rate': 0.1689673689977081}\nGRU - MSE: 21.28, Accuracy Percentage: -1978.56%\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "x7K1Q9mh67ah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d1438b-3785-4f71-fee0-1135881a0a95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cross Validation (K-Fold, Stratified K-fold, Repeated K-fold)**"
      ],
      "metadata": {
        "id": "6prruL7exeXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, RepeatedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# ============================\n",
        "# Load the Iris dataset\n",
        "# ============================\n",
        "data = load_iris()  # Load the Iris dataset from sklearn\n",
        "X = data.data  # Features (sepal length, sepal width, etc.)\n",
        "y = data.target  # Target variable (species of iris)\n",
        "\n",
        "# ============================\n",
        "# Scale features\n",
        "# ============================\n",
        "scaler = StandardScaler()  # Initialize the scaler\n",
        "X_scaled = scaler.fit_transform(X)  # Scale features to have mean=0 and variance=1\n",
        "\n",
        "# ============================\n",
        "# Function to create the ANN model\n",
        "# ============================\n",
        "def create_ann_model():\n",
        "    model = Sequential()  # Initialize a sequential model\n",
        "    model.add(Dense(10, activation='relu', input_shape=(X_scaled.shape[1],)))  # Hidden layer with 10 neurons\n",
        "    model.add(Dense(3, activation='softmax'))  # Output layer with 3 neurons (for 3 classes) using softmax activation\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "    return model  # Return the created model\n",
        "\n",
        "# ============================\n",
        "# K-Fold Cross-Validation\n",
        "# ============================\n",
        "def k_fold_cross_validation(X, y, n_splits=5):\n",
        "    kf = KFold(n_splits=n_splits)  # Initialize K-Fold cross-validation\n",
        "    accuracies = []  # List to store accuracies for each fold\n",
        "\n",
        "    # Iterate through each fold\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]  # Split the data into training and testing sets\n",
        "        y_train, y_test = y[train_index], y[test_index]  # Split the labels accordingly\n",
        "\n",
        "        model = create_ann_model()  # Create a new ANN model\n",
        "        model.fit(X_train, y_train, epochs=50, batch_size=5, verbose=0)  # Train the model\n",
        "        y_pred = np.argmax(model.predict(X_test), axis=1)  # Make predictions on the test set\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))  # Calculate accuracy and store it\n",
        "\n",
        "    print(\"K-Fold Cross-Validation Accuracy: \", np.mean(accuracies))  # Print the average accuracy\n",
        "\n",
        "# ============================\n",
        "# Stratified K-Fold Cross-Validation\n",
        "# ============================\n",
        "def stratified_k_fold_cross_validation(X, y, n_splits=5):\n",
        "    skf = StratifiedKFold(n_splits=n_splits)  # Initialize Stratified K-Fold cross-validation\n",
        "    accuracies = []  # List to store accuracies for each fold\n",
        "\n",
        "    # Iterate through each fold\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]  # Split the data into training and testing sets\n",
        "        y_train, y_test = y[train_index], y[test_index]  # Split the labels accordingly\n",
        "\n",
        "        model = create_ann_model()  # Create a new ANN model\n",
        "        model.fit(X_train, y_train, epochs=50, batch_size=5, verbose=0)  # Train the model\n",
        "        y_pred = np.argmax(model.predict(X_test), axis=1)  # Make predictions on the test set\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))  # Calculate accuracy and store it\n",
        "\n",
        "    print(\"Stratified K-Fold Cross-Validation Accuracy: \", np.mean(accuracies))  # Print the average accuracy\n",
        "\n",
        "# ============================\n",
        "# Repeated K-Fold Cross-Validation\n",
        "# ============================\n",
        "def repeated_k_fold_cross_validation(X, y, n_splits=5, n_repeats=3):\n",
        "    rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats)  # Initialize Repeated K-Fold cross-validation\n",
        "    accuracies = []  # List to store accuracies for each fold\n",
        "\n",
        "    # Iterate through each fold\n",
        "    for train_index, test_index in rkf.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]  # Split the data into training and testing sets\n",
        "        y_train, y_test = y[train_index], y[test_index]  # Split the labels accordingly\n",
        "\n",
        "        model = create_ann_model()  # Create a new ANN model\n",
        "        model.fit(X_train, y_train, epochs=50, batch_size=5, verbose=0)  # Train the model\n",
        "        y_pred = np.argmax(model.predict(X_test), axis=1)  # Make predictions on the test set\n",
        "        accuracies.append(accuracy_score(y_test, y_pred))  # Calculate accuracy and store it\n",
        "\n",
        "    print(\"Repeated K-Fold Cross-Validation Accuracy: \", np.mean(accuracies))  # Print the average accuracy\n",
        "\n",
        "# ============================\n",
        "# Run cross-validation methods\n",
        "# ============================\n",
        "k_fold_cross_validation(X_scaled, y)  # Execute K-Fold cross-validation\n",
        "stratified_k_fold_cross_validation(X_scaled, y)  # Execute Stratified K-Fold cross-validation\n",
        "repeated_k_fold_cross_validation(X_scaled, y)  # Execute Repeated K-Fold cross-validation\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1/1 [==============================] - 0s 175ms/step\n1/1 [==============================] - 0s 231ms/step\n1/1 [==============================] - 0s 56ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "WARNING:tensorflow:5 out of the last 133 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7d26b1412290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1/1 [==============================] - 0s 145ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "WARNING:tensorflow:6 out of the last 134 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7d26b0adc5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1/1 [==============================] - 0s 136ms/step\nK-Fold Cross-Validation Accuracy:  0.8200000000000001\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 83ms/step\n1/1 [==============================] - 0s 90ms/step\n1/1 [==============================] - 0s 75ms/step\n1/1 [==============================] - 0s 86ms/step\nStratified K-Fold Cross-Validation Accuracy:  0.9199999999999999\n1/1 [==============================] - 0s 134ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 158ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 1s 587ms/step\n1/1 [==============================] - 0s 438ms/step\n1/1 [==============================] - 0s 224ms/step\n1/1 [==============================] - 0s 160ms/step\n1/1 [==============================] - 0s 278ms/step\n1/1 [==============================] - 0s 138ms/step\n1/1 [==============================] - 0s 149ms/step\n1/1 [==============================] - 0s 98ms/step\n1/1 [==============================] - 0s 57ms/step\n1/1 [==============================] - 0s 56ms/step\n1/1 [==============================] - 0s 59ms/step\nRepeated K-Fold Cross-Validation Accuracy:  0.9311111111111112\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAeF5LFzwyti",
        "outputId": "32e7807e-e81a-4e2c-dd3f-0b097e0fd61a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target variable (housing prices)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data (important for ANN)\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train = scaler_X.fit_transform(X_train)\n",
        "X_test = scaler_X.transform(X_test)\n",
        "\n",
        "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "y_test = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# Define the ANN model\n",
        "def create_ann_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer with 64 neurons and ReLU activation\n",
        "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "\n",
        "    # Hidden layer with 32 neurons and ReLU activation\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "\n",
        "    # Output layer with 1 neuron (regression output)\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Initialize and view the model summary\n",
        "model = create_ann_model()\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Wrap the model in a KerasRegressor for use in GridSearchCV\n",
        "model = KerasRegressor(build_fn=create_ann_model, verbose=0)\n",
        "\n",
        "# Define the grid of hyperparameters to search\n",
        "param_grid = {\n",
        "    'batch_size': [16, 32, 64],\n",
        "    'epochs': [50, 100]\n",
        "}\n",
        "\n",
        "# Perform Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and results\n",
        "print(f\"Best Parameters: {grid_result.best_params_}\")\n",
        "print(f\"Best MSE: {-grid_result.best_score_:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "import optuna\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# Objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Define hyperparameters to tune\n",
        "    num_neurons = trial.suggest_int('num_neurons', 16, 128)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)\n",
        "\n",
        "    # Create the model with suggested hyperparameters\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_neurons, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(num_neurons//2, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    return mse\n",
        "\n",
        "# Run the optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# Best parameters from Optuna\n",
        "print(f\"Best hyperparameters: {study.best_params}\")\n",
        "print(f\"Best MSE: {study.best_value:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Train the model using the best hyperparameters from GridSearch or Optuna\n",
        "best_model = create_ann_model()\n",
        "best_model.fit(X_train, y_train, epochs=grid_result.best_params_['epochs'], batch_size=grid_result.best_params_['batch_size'], verbose=1)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Inverse transform the scaled target variable\n",
        "y_pred = scaler_y.inverse_transform(y_pred)\n",
        "y_test_orig = scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test_orig, y_pred)\n",
        "r2 = r2_score(y_test_orig, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model: \"sequential_37\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_62 (Dense)            (None, 64)                576       \n                                                                 \n dense_63 (Dense)            (None, 32)                2080      \n                                                                 \n dense_64 (Dense)            (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 2,689\nTrainable params: 2,689\nNon-trainable params: 0\n_________________________________________________________________\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "<ipython-input-3-379aace86f22>:59: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n  model = KerasRegressor(build_fn=create_ann_model, verbose=0)\n[I 2024-09-30 17:27:53,584] A new study created in memory with name: no-name-c2a5e5c5-5ddc-46f3-a4a2-0c1b4bef1367\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Best Parameters: {'batch_size': 16, 'epochs': 100}\nBest MSE: 0.2120\n129/129 [==============================] - 0s 1ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 17:28:43,165] Trial 0 finished with value: 0.21475482409303853 and parameters: {'num_neurons': 98, 'dropout_rate': 0.32701832963603594, 'learning_rate': 0.0049096427666714074}. Best is trial 0 with value: 0.21475482409303853.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "129/129 [==============================] - 1s 2ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 17:30:07,253] Trial 1 finished with value: 0.20047890977020366 and parameters: {'num_neurons': 91, 'dropout_rate': 0.20291022952264212, 'learning_rate': 0.004113362635548157}. Best is trial 1 with value: 0.20047890977020366.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "129/129 [==============================] - 0s 3ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 17:30:57,235] Trial 2 finished with value: 0.24324740619391835 and parameters: {'num_neurons': 33, 'dropout_rate': 0.25408584464508177, 'learning_rate': 0.009955869172027365}. Best is trial 1 with value: 0.20047890977020366.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "129/129 [==============================] - 0s 2ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 17:32:20,445] Trial 3 finished with value: 0.23992480140778294 and parameters: {'num_neurons': 17, 'dropout_rate': 0.16259696052354816, 'learning_rate': 0.006385724762526814}. Best is trial 1 with value: 0.20047890977020366.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "129/129 [==============================] - 0s 3ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 17:33:44,097] Trial 4 finished with value: 0.19405087059895726 and parameters: {'num_neurons': 114, 'dropout_rate': 0.2513072437706121, 'learning_rate': 0.0025027069220994167}. Best is trial 4 with value: 0.19405087059895726.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "129/129 [==============================] - 0s 2ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 17:34:36,318] Trial 5 finished with value: 0.2283901214781765 and parameters: {'num_neurons': 99, 'dropout_rate': 0.4961610834389195, 'learning_rate': 0.009435458933007918}. Best is trial 4 with value: 0.19405087059895726.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "129/129 [==============================] - 0s 1ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 17:35:26,559] Trial 6 finished with value: 0.236042064772658 and parameters: {'num_neurons': 34, 'dropout_rate': 0.32766867572194147, 'learning_rate': 0.0023287774129797587}. Best is trial 4 with value: 0.19405087059895726.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "129/129 [==============================] - 0s 2ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 17:36:50,103] Trial 7 finished with value: 0.23358207009924084 and parameters: {'num_neurons': 82, 'dropout_rate': 0.4274647478520026, 'learning_rate': 0.007733974994649547}. Best is trial 4 with value: 0.19405087059895726.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "129/129 [==============================] - 0s 2ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 17:38:13,829] Trial 8 finished with value: 0.23294319970662566 and parameters: {'num_neurons': 92, 'dropout_rate': 0.48620873757627836, 'learning_rate': 0.008202627498596298}. Best is trial 4 with value: 0.19405087059895726.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "129/129 [==============================] - 0s 2ms/step\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[I 2024-09-30 17:39:37,030] Trial 9 finished with value: 0.20931085855701523 and parameters: {'num_neurons': 52, 'dropout_rate': 0.16347466088343315, 'learning_rate': 0.003246329370839119}. Best is trial 4 with value: 0.19405087059895726.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Best hyperparameters: {'num_neurons': 114, 'dropout_rate': 0.2513072437706121, 'learning_rate': 0.0025027069220994167}\nBest MSE: 0.1941\nEpoch 1/100\n1032/1032 [==============================] - 4s 3ms/step - loss: 0.3862\nEpoch 2/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.2972\nEpoch 3/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.2577\nEpoch 4/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.2668\nEpoch 5/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.2327\nEpoch 6/100\n1032/1032 [==============================] - 4s 3ms/step - loss: 0.2317\nEpoch 7/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.2491\nEpoch 8/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.2248\nEpoch 9/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.2150\nEpoch 10/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.2117\nEpoch 11/100\n1032/1032 [==============================] - 4s 4ms/step - loss: 0.2133\nEpoch 12/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.2071\nEpoch 13/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.2079\nEpoch 14/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.2056\nEpoch 15/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.2024\nEpoch 16/100\n1032/1032 [==============================] - 4s 4ms/step - loss: 0.2027\nEpoch 17/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.2000\nEpoch 18/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1983\nEpoch 19/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1987\nEpoch 20/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1962\nEpoch 21/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1959\nEpoch 22/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1940\nEpoch 23/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1936\nEpoch 24/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1922\nEpoch 25/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1915\nEpoch 26/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1900\nEpoch 27/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1894\nEpoch 28/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1882\nEpoch 29/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1883\nEpoch 30/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1868\nEpoch 31/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1874\nEpoch 32/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1859\nEpoch 33/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1865\nEpoch 34/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1854\nEpoch 35/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1842\nEpoch 36/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1835\nEpoch 37/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1841\nEpoch 38/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1833\nEpoch 39/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1826\nEpoch 40/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1811\nEpoch 41/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1814\nEpoch 42/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1804\nEpoch 43/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1808\nEpoch 44/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1803\nEpoch 45/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1795\nEpoch 46/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1783\nEpoch 47/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1776\nEpoch 48/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1782\nEpoch 49/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1768\nEpoch 50/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1768\nEpoch 51/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1760\nEpoch 52/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1747\nEpoch 53/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1755\nEpoch 54/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1751\nEpoch 55/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1729\nEpoch 56/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1750\nEpoch 57/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1742\nEpoch 58/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1731\nEpoch 59/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1729\nEpoch 60/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1711\nEpoch 61/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1712\nEpoch 62/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1712\nEpoch 63/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1706\nEpoch 64/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1702\nEpoch 65/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1681\nEpoch 66/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1691\nEpoch 67/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1686\nEpoch 68/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1692\nEpoch 69/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1670\nEpoch 70/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1674\nEpoch 71/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1696\nEpoch 72/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1749\nEpoch 73/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1666\nEpoch 74/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1667\nEpoch 75/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1662\nEpoch 76/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1647\nEpoch 77/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1643\nEpoch 78/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1646\nEpoch 79/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1648\nEpoch 80/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1644\nEpoch 81/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1635\nEpoch 82/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1627\nEpoch 83/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1636\nEpoch 84/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1628\nEpoch 85/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1717\nEpoch 86/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1624\nEpoch 87/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1617\nEpoch 88/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1611\nEpoch 89/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1606\nEpoch 90/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1615\nEpoch 91/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1606\nEpoch 92/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1602\nEpoch 93/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1621\nEpoch 94/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1597\nEpoch 95/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1596\nEpoch 96/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1584\nEpoch 97/100\n1032/1032 [==============================] - 3s 3ms/step - loss: 0.1597\nEpoch 98/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1581\nEpoch 99/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1592\nEpoch 100/100\n1032/1032 [==============================] - 2s 2ms/step - loss: 0.1581\n129/129 [==============================] - 0s 1ms/step\nMean Squared Error: 0.2650\nR² Score: 0.7978\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEqno4jGy6QC",
        "outputId": "d74119af-7d98-44a6-b0b6-db2d6329e5ff"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "39z7ohfM5YHy"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python",
      "language": "python",
      "display_name": "Pyolite (preview)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernel_info": {
      "name": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}